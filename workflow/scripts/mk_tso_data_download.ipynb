{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676e2466",
   "metadata": {},
   "source": [
    "**Code to download hourly electricity demand data from the North Macedonian TSO (MEPSO).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://www.mepso.com.mk/files/mk/dnevni/Информација за {date}.pdf\"\n",
    "SAVE_DIR = \"pdfs\"\n",
    "LOG_DIR = \"logs\"\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 1, 10)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "all_data = []\n",
    "error_log = []\n",
    "\n",
    "# Extract values from text\n",
    "def extract_vkupen_konzum_from_text(text):\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if \"ВКУПЕН КОНЗУМ\" in line:\n",
    "            values = re.findall(r\"\\d{1,3}(?:\\.\\d{3})*,\\d+|\\d+,\\d+\", line)\n",
    "            numbers = [\n",
    "                float(v.replace(\".\", \"\").replace(\",\", \".\"))\n",
    "                for v in values\n",
    "            ]\n",
    "            if len(numbers) >= 25:\n",
    "                return numbers[1:25]  # skip total, return hourly\n",
    "            elif len(numbers) >= 24:\n",
    "                return numbers[:24]\n",
    "    return None\n",
    "\n",
    "# Main processing function\n",
    "def process_date(date_obj):\n",
    "    date_str = date_obj.strftime(\"%d.%m.%Y\")\n",
    "    url = BASE_URL.format(date=date_str)\n",
    "    filename = os.path.join(SAVE_DIR, f\"Информација за {date_str}.pdf\")\n",
    "    fallback_txt_path = os.path.join(LOG_DIR, f\"fallback_{date_str}.txt\")\n",
    "\n",
    "    try:\n",
    "        # Download PDF\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            error_log.append((date_str, \"Download failed\"))\n",
    "            return\n",
    "\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Try extracting from table\n",
    "        with pdfplumber.open(filename) as pdf:\n",
    "            page = pdf.pages[0]\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    if row and any(\"ВКУПЕН КОНЗУМ\" in str(cell) for cell in row if cell):\n",
    "                        values = [\n",
    "                            float(str(cell).replace(\".\", \"\").replace(\",\", \".\").replace(\" \", \"\"))\n",
    "                            for cell in row\n",
    "                            if cell and str(cell).replace(\".\", \"\").replace(\",\", \"\").replace(\" \", \"\").isdigit()\n",
    "                        ]\n",
    "                        if len(values) >= 25:\n",
    "                            hourly = values[1:25]\n",
    "                            for h, v in enumerate(hourly, start=1):\n",
    "                                all_data.append({\n",
    "                                    \"date\": date_obj.date().isoformat(),\n",
    "                                    \"hour\": h,\n",
    "                                    \"value\": v\n",
    "                                })\n",
    "                            return\n",
    "\n",
    "        # Fallback: extract text and save for debugging\n",
    "        with pdfplumber.open(filename) as pdf:\n",
    "            page = pdf.pages[0]\n",
    "            text = page.extract_text() or \"\"\n",
    "            with open(fallback_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            hourly = extract_vkupen_konzum_from_text(text)\n",
    "            if hourly:\n",
    "                for h, v in enumerate(hourly, start=1):\n",
    "                    all_data.append({\n",
    "                        \"date\": date_obj.date().isoformat(),\n",
    "                        \"hour\": h,\n",
    "                        \"value\": v\n",
    "                    })\n",
    "                return\n",
    "            else:\n",
    "                error_log.append((date_str, f\"Too few values in direct text ({len(hourly) if hourly else 0})\"))\n",
    "\n",
    "        # FINAL fallback: Load from saved .txt file\n",
    "        if os.path.exists(fallback_txt_path):\n",
    "            with open(fallback_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                hourly = extract_vkupen_konzum_from_text(text)\n",
    "                if hourly:\n",
    "                    for h, v in enumerate(hourly, start=1):\n",
    "                        all_data.append({\n",
    "                            \"date\": date_obj.date().isoformat(),\n",
    "                            \"hour\": h,\n",
    "                            \"value\": v\n",
    "                        })\n",
    "                    return\n",
    "                else:\n",
    "                    error_log.append((date_str, f\"Too few values in saved .txt ({len(hourly) if hourly else 0})\"))\n",
    "        else:\n",
    "            error_log.append((date_str, \"No fallback file found\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append((date_str, f\"Exception: {e}\"))\n",
    "\n",
    "# process date range\n",
    "dates = [\n",
    "    START_DATE + timedelta(days=i)\n",
    "    for i in range((END_DATE - START_DATE).days + 1)\n",
    "    if not ((START_DATE + timedelta(days=i)).month == 2 and (START_DATE + timedelta(days=i)).day == 29)\n",
    "]\n",
    "\n",
    "print(\"Processing...\")\n",
    "for d in tqdm(dates):\n",
    "    process_date(d)\n",
    "\n",
    "# Save outputs\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_excel(\"mk_tso_data_hourly_demand.xlsx\", index=False)\n",
    "\n",
    "if error_log:\n",
    "    pd.DataFrame(error_log, columns=[\"date\", \"issue\"]).to_csv(\"error_log.csv\", index=False)\n",
    "\n",
    "print(\"Done, data extracted and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300cae83",
   "metadata": {},
   "source": [
    "**An alternative version of the code that utilizes parallel downloading to accelerate the data retrieval process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://www.mepso.com.mk/files/mk/dnevni/Информација за {date}.pdf\"\n",
    "SAVE_DIR = \"pdfs\"\n",
    "LOG_DIR = \"logs\"\n",
    "COMPLETED_FILE = \"completed_dates.txt\"\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 1, 15)\n",
    "MAX_RETRIES = 2\n",
    "RETRY_DELAY = 5  # seconds\n",
    "MAX_WORKERS = 8  # number of threads\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "all_data = []\n",
    "error_log = []\n",
    "\n",
    "# Helper function to clean and convert numbers\n",
    "def clean_number(s):\n",
    "    return float(s.replace(\".\", \"\").replace(\",\", \".\"))\n",
    "\n",
    "# Extract values from text\n",
    "def extract_vkupen_konzum_from_text(text):\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if \"ВКУПЕН КОНЗУМ\" in line:\n",
    "            values = re.findall(r\"\\d{1,3}(?:\\.\\d{3})*,\\d+|\\d+,\\d+\", line)\n",
    "            numbers = [clean_number(v) for v in values]\n",
    "            if len(numbers) >= 25:\n",
    "                return numbers[1:25]\n",
    "            elif len(numbers) >= 24:\n",
    "                return numbers[:24]\n",
    "    return None\n",
    "\n",
    "# Download file with retries\n",
    "def download_file(url, filename):\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Attempt {attempt}: Server returned status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt}: Exception during download: {e}\")\n",
    "\n",
    "        time.sleep(RETRY_DELAY)\n",
    "    return False\n",
    "\n",
    "# Save completed date\n",
    "def mark_completed(date_obj):\n",
    "    with open(COMPLETED_FILE, \"a\") as f:\n",
    "        f.write(date_obj.strftime(\"%Y-%m-%d\") + \"\\n\")\n",
    "\n",
    "# Check completed dates\n",
    "def load_completed_dates():\n",
    "    if not os.path.exists(COMPLETED_FILE):\n",
    "        return set()\n",
    "    with open(COMPLETED_FILE, \"r\") as f:\n",
    "        return set(line.strip() for line in f.readlines())\n",
    "\n",
    "# Main processing function\n",
    "def process_date(date_obj):\n",
    "    date_str = date_obj.strftime(\"%d.%m.%Y\")\n",
    "    date_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if date_iso in completed_dates:\n",
    "        # Already completed\n",
    "        return\n",
    "    \n",
    "    url = BASE_URL.format(date=date_str)\n",
    "    filename = os.path.join(SAVE_DIR, f\"Информација за {date_str}.pdf\")\n",
    "    fallback_txt_path = os.path.join(LOG_DIR, f\"fallback_{date_str}.txt\")\n",
    "\n",
    "    try:\n",
    "        if not download_file(url, filename):\n",
    "            error_log.append((date_str, \"Download failed after retries\"))\n",
    "            return\n",
    "\n",
    "        with pdfplumber.open(filename) as pdf:\n",
    "            page = pdf.pages[0]\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                for row in table:\n",
    "                    if row and any(\"ВКУПЕН КОНЗУМ\" in str(cell) for cell in row if cell):\n",
    "                        values = [\n",
    "                            clean_number(str(cell))\n",
    "                            for cell in row\n",
    "                            if cell and re.match(r\"\\d{1,3}(?:\\.\\d{3})*,\\d+|\\d+,\\d+\", str(cell))\n",
    "                        ]\n",
    "                        if len(values) >= 25:\n",
    "                            hourly = values[1:25]\n",
    "                            for h, v in enumerate(hourly, start=1):\n",
    "                                all_data.append({\"date\": date_obj.date().isoformat(), \"hour\": h, \"value\": v})\n",
    "                            mark_completed(date_obj)\n",
    "                            return\n",
    "\n",
    "        # Fallback: extract text if table fails\n",
    "        with pdfplumber.open(filename) as pdf:\n",
    "            page = pdf.pages[0]\n",
    "            text = page.extract_text() or \"\"\n",
    "            with open(fallback_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            hourly = extract_vkupen_konzum_from_text(text)\n",
    "            if hourly:\n",
    "                for h, v in enumerate(hourly, start=1):\n",
    "                    all_data.append({\"date\": date_obj.date().isoformat(), \"hour\": h, \"value\": v})\n",
    "                mark_completed(date_obj)\n",
    "                return\n",
    "            else:\n",
    "                error_log.append((date_str, f\"Too few values in direct text ({len(hourly) if hourly else 0})\"))\n",
    "\n",
    "        # FINAL fallback\n",
    "        if os.path.exists(fallback_txt_path):\n",
    "            with open(fallback_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                hourly = extract_vkupen_konzum_from_text(text)\n",
    "                if hourly:\n",
    "                    for h, v in enumerate(hourly, start=1):\n",
    "                        all_data.append({\"date\": date_obj.date().isoformat(), \"hour\": h, \"value\": v})\n",
    "                    mark_completed(date_obj)\n",
    "                    return\n",
    "                else:\n",
    "                    error_log.append((date_str, f\"Too few values in saved .txt ({len(hourly) if hourly else 0})\"))\n",
    "        else:\n",
    "            error_log.append((date_str, \"No fallback file found\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append((date_str, f\"Exception: {e}\"))\n",
    "\n",
    "# Load completed dates\n",
    "completed_dates = load_completed_dates()\n",
    "\n",
    "# Generate list of dates to process\n",
    "dates = [\n",
    "    START_DATE + timedelta(days=i)\n",
    "    for i in range((END_DATE - START_DATE).days + 1)\n",
    "    if not ((START_DATE + timedelta(days=i)).month == 2 and (START_DATE + timedelta(days=i)).day == 29)\n",
    "]\n",
    "\n",
    "# Parallel processing\n",
    "print(f\"Processing {len(dates)} dates with {MAX_WORKERS} workers...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_date, d): d for d in dates}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        pass\n",
    "\n",
    "# Save outputs\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_excel(\"mk_tso_data_hourly_demand.xlsx\", index=False)\n",
    "\n",
    "if error_log:\n",
    "    pd.DataFrame(error_log, columns=[\"date\", \"issue\"]).to_csv(\"error_log.csv\", index=False)\n",
    "\n",
    "print(\"Done, data extracted and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osembe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
