{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676e2466",
   "metadata": {},
   "source": [
    "**Code to download hourly electricity demand data from the North Macedonian TSO (MEPSO).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import contextlib\n",
    "import nest_asyncio\n",
    "\n",
    "# Download configuration\n",
    "BASE_URL = \"https://www.mepso.com.mk/files/mk/dnevni/Информација за {date}.pdf\"\n",
    "SAVE_DIR = \"pdfs\"\n",
    "LOG_DIR = \"logs\"\n",
    "OUTPUT_FILE = \"mk_tso_data_hourly_demand.xlsx\"\n",
    "START_DATE = datetime(2024, 12, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "#END_DATE = datetime.now().strftime(\"%d.%m.%Y\")  # Today's date\n",
    "\n",
    "MAX_PARALLEL_DOWNLOADS = 10  # Adjust this depending on internet speed\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Helper to suppress PDFMiner noisy stderr\n",
    "@contextlib.contextmanager\n",
    "def suppress_stderr():\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stderr = fnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "# Extract numbers from text fallback\n",
    "def extract_vkupen_konzum_from_text(text):\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if \"ВКУПЕН КОНЗУМ\" in line:\n",
    "            values = re.findall(r\"\\d{1,3}(?:\\.\\d{3})*,\\d+|\\d+,\\d+\", line)\n",
    "            numbers = [\n",
    "                float(v.replace(\".\", \"\").replace(\",\", \".\")) for v in values\n",
    "            ]\n",
    "            if len(numbers) >= 25:\n",
    "                return numbers[1:25]\n",
    "            elif len(numbers) >= 24:\n",
    "                return numbers[:24]\n",
    "    return None\n",
    "\n",
    "# Single Date Processor\n",
    "async def process_date(session, date_obj, all_data, error_log):\n",
    "    date_str = date_obj.strftime(\"%d.%m.%Y\")\n",
    "    url = BASE_URL.format(date=date_str)\n",
    "    filename = os.path.join(SAVE_DIR, f\"Информација за {date_str}.pdf\")\n",
    "    fallback_txt_path = os.path.join(LOG_DIR, f\"fallback_{date_str}.txt\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as resp:\n",
    "                if resp.status != 200:\n",
    "                    error_log.append((date_str, \"Download failed\"))\n",
    "                    return\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(await resp.read())\n",
    "\n",
    "        # Try extracting from table\n",
    "        with suppress_stderr():\n",
    "            with pdfplumber.open(filename) as pdf:\n",
    "                page = pdf.pages[0]\n",
    "                tables = page.extract_tables()\n",
    "                for table in tables:\n",
    "                    for row in table:\n",
    "                        if row and any(\"ВКУПЕН КОНЗУМ\" in str(cell) for cell in row if cell):\n",
    "                            values = [\n",
    "                                float(str(cell).replace(\".\", \"\").replace(\",\", \".\").replace(\" \", \"\"))\n",
    "                                for cell in row\n",
    "                                if cell and str(cell).replace(\".\", \"\").replace(\",\", \"\").replace(\" \", \"\").isdigit()\n",
    "                            ]\n",
    "                            if len(values) >= 25:\n",
    "                                hourly = values[1:25]\n",
    "                                for h, v in enumerate(hourly, start=1):\n",
    "                                    all_data.append({\"date\": date_obj.date().isoformat(), \"hour\": h, \"value\": v})\n",
    "                                return\n",
    "\n",
    "        # Fallback: extract from text\n",
    "        with suppress_stderr():\n",
    "            with pdfplumber.open(filename) as pdf:\n",
    "                page = pdf.pages[0]\n",
    "                text = page.extract_text() or \"\"\n",
    "\n",
    "            with open(fallback_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            hourly = extract_vkupen_konzum_from_text(text)\n",
    "            if hourly:\n",
    "                for h, v in enumerate(hourly, start=1):\n",
    "                    all_data.append({\"date\": date_obj.date().isoformat(), \"hour\": h, \"value\": v})\n",
    "                return\n",
    "            else:\n",
    "                error_log.append((date_str, f\"Too few values extracted\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log.append((date_str, f\"Exception: {e}\"))\n",
    "\n",
    "# Main Async Runner\n",
    "async def main():\n",
    "    dates = [\n",
    "        START_DATE + timedelta(days=i)\n",
    "        for i in range((END_DATE - START_DATE).days + 1)\n",
    "        if not ((START_DATE + timedelta(days=i)).month == 2 and (START_DATE + timedelta(days=i)).day == 29)\n",
    "    ]\n",
    "\n",
    "    all_data = []\n",
    "    error_log = []\n",
    "\n",
    "    print(\"Processing...\")\n",
    "    connector = aiohttp.TCPConnector(limit_per_host=MAX_PARALLEL_DOWNLOADS)\n",
    "    timeout = aiohttp.ClientTimeout(total=30)\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        tasks = []\n",
    "        pbar = tqdm(total=len(dates), desc=\"Downloading & Extracting\")\n",
    "\n",
    "        sem = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)\n",
    "\n",
    "        async def sem_task(date):\n",
    "            async with sem:\n",
    "                await process_date(session, date, all_data, error_log)\n",
    "                pbar.update(1)\n",
    "\n",
    "        for date in dates:\n",
    "            tasks.append(asyncio.create_task(sem_task(date)))\n",
    "\n",
    "        await asyncio.gather(*tasks)\n",
    "        pbar.close()\n",
    "\n",
    "    # Save results\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df = df.sort_values(by=[\"date\", \"hour\"]).reset_index(drop=True)\n",
    "        df.to_excel(OUTPUT_FILE, index=False)\n",
    "        print(f\"Data saved to {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"No data was downloaded. Excel file was not created.\")\n",
    "\n",
    "    # Save error log if any\n",
    "    if error_log:\n",
    "        pd.DataFrame(error_log, columns=[\"date\", \"issue\"]).to_csv(\"error_log.csv\", index=False)\n",
    "        print(f\"Errors encountered. See error_log.csv for details.\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "# Runner (can be modified if the file is ran as a .py file instead of a .ipynb)\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osembe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
